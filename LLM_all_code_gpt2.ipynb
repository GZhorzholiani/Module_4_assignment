{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "mount_file_id": "1OfRYwu3wKjCIqVNl94-VSvDtHu05ZXyy",
      "authorship_tag": "ABX9TyM2WSE3SKXrIQj9HiPimhJx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GZhorzholiani/Module_4_assignment/blob/main/LLM_all_code_gpt2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SGpDKGlBzNWH",
        "outputId": "e339b9f6-7cb6-482f-ff6d-1311f598d4f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "tiktoken version :  0.9.0\n",
            "CUDA available: True\n",
            "Device name: NVIDIA A100-SXM4-40GB\n",
            "Device: cuda\n",
            "Train loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "\n",
            "Validation loader:\n",
            "torch.Size([2, 256]) torch.Size([2, 256])\n",
            "9\n",
            "1\n",
            "Training tokens: 4608\n",
            "Validation tokens: 512\n",
            "All tokens: 5120\n",
            "Ep 1 (Step 000000): Train loss 9.919, Val loss 10.064\n",
            "Ep 1 (Step 000005): Train loss 7.935, Val loss 8.277\n",
            "Every effort moves you,,,,,,,,,,,,.                                     \n",
            "Ep 2 (Step 000010): Train loss 6.495, Val loss 7.007\n",
            "Ep 2 (Step 000015): Train loss 5.889, Val loss 6.531\n",
            "Every effort moves you, and, and the           \", and, and, and, and, and, and the, and the, and, and, and, and the, and the, and, and,\n",
            "Ep 3 (Step 000020): Train loss 5.040, Val loss 6.371\n",
            "Ep 3 (Step 000025): Train loss 4.380, Val loss 6.325\n",
            "Every effort moves you.               \"I was not        \"I was his pictures--and it. Gisburn, and I was not of the picture. G\n",
            "Ep 4 (Step 000030): Train loss 3.753, Val loss 6.193\n",
            "Ep 4 (Step 000035): Train loss 3.442, Val loss 6.156\n",
            "Every effort moves you know the, and in the picture.                                          \n",
            "Ep 5 (Step 000040): Train loss 2.673, Val loss 6.163\n",
            "Every effort moves you know he was not that I felt it was \"I had a me in a so that he was a of the fact, the her poverty.                     \n",
            "Ep 6 (Step 000045): Train loss 2.322, Val loss 6.204\n",
            "Ep 6 (Step 000050): Train loss 1.807, Val loss 6.192\n",
            "Every effort moves you?\"  \"--as, he was \"interesting\": on the last word.        \"Oh, in the moment--as Jack himself, one might put it, the donkey. \"There were days when I\n",
            "Ep 7 (Step 000055): Train loss 1.621, Val loss 6.282\n",
            "Ep 7 (Step 000060): Train loss 1.066, Val loss 6.259\n",
            "Every effort moves you?\"  \"Yes--I glanced after him, so inevitably the last word. \"--I looked up, I felt to see a smile behind his pictures.  \"I to have him, I was down the room, when I\n",
            "Ep 8 (Step 000065): Train loss 0.762, Val loss 6.367\n",
            "Ep 8 (Step 000070): Train loss 0.606, Val loss 6.417\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 9 (Step 000075): Train loss 0.440, Val loss 6.452\n",
            "Ep 9 (Step 000080): Train loss 0.317, Val loss 6.521\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the fact with equanimity. Victor Grindle was, in fact, and that, in the moment--as Jack himself, one might put it, and _jardiniere_ full of\n",
            "Ep 10 (Step 000085): Train loss 0.223, Val loss 6.635\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--she's an awful simpleton, you know, and threw back his glory, he had dropped his painting, had been the man of the hour. The\n",
            "Ep 11 (Step 000090): Train loss 0.178, Val loss 6.726\n",
            "Ep 11 (Step 000095): Train loss 0.157, Val loss 6.768\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  Mrs. It was just because she was _not_ interesting--if I saw that, when Stroud laid in the first\n",
            "Ep 12 (Step 000100): Train loss 0.140, Val loss 6.868\n",
            "Ep 12 (Step 000105): Train loss 0.135, Val loss 6.851\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his glory, he had dropped his painting, had been the man of the hour. The\n",
            "Ep 13 (Step 000110): Train loss 0.147, Val loss 6.956\n",
            "Ep 13 (Step 000115): Train loss 0.136, Val loss 6.927\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--she's past! The women had made him--it was fitting that they should mourn him. Among his own sex fewer regrets were heard, and in his\n",
            "Ep 14 (Step 000120): Train loss 0.102, Val loss 7.064\n",
            "Ep 14 (Step 000125): Train loss 0.090, Val loss 7.056\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 15 (Step 000130): Train loss 0.051, Val loss 7.058\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  Mrs. Gisburn drew back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 16 (Step 000135): Train loss 0.067, Val loss 7.126\n",
            "Ep 16 (Step 000140): Train loss 0.045, Val loss 7.105\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 17 (Step 000145): Train loss 0.045, Val loss 7.116\n",
            "Ep 17 (Step 000150): Train loss 0.045, Val loss 7.137\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 18 (Step 000155): Train loss 0.034, Val loss 7.150\n",
            "Ep 18 (Step 000160): Train loss 0.025, Val loss 7.240\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 19 (Step 000165): Train loss 0.027, Val loss 7.216\n",
            "Ep 19 (Step 000170): Train loss 0.017, Val loss 7.288\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 20 (Step 000175): Train loss 0.050, Val loss 7.252\n",
            "Every effort moves you?\" He, and pushed one of the deep arm-chairs forward. \"There: make yourself comfortable--and here are the cigars you like.\" \"I looked at the donkey again. I saw that, when Stroud laid in the first\n",
            "Ep 21 (Step 000180): Train loss 0.034, Val loss 7.202\n",
            "Ep 21 (Step 000185): Train loss 0.021, Val loss 7.255\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 22 (Step 000190): Train loss 0.025, Val loss 7.383\n",
            "Ep 22 (Step 000195): Train loss 0.014, Val loss 7.330\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 23 (Step 000200): Train loss 0.013, Val loss 7.365\n",
            "Ep 23 (Step 000205): Train loss 0.019, Val loss 7.310\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 24 (Step 000210): Train loss 0.015, Val loss 7.331\n",
            "Ep 24 (Step 000215): Train loss 0.021, Val loss 7.442\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 25 (Step 000220): Train loss 0.008, Val loss 7.423\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 26 (Step 000225): Train loss 0.006, Val loss 7.453\n",
            "Ep 26 (Step 000230): Train loss 0.005, Val loss 7.489\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 27 (Step 000235): Train loss 0.006, Val loss 7.484\n",
            "Ep 27 (Step 000240): Train loss 0.005, Val loss 7.480\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 28 (Step 000245): Train loss 0.006, Val loss 7.469\n",
            "Ep 28 (Step 000250): Train loss 0.004, Val loss 7.501\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 29 (Step 000255): Train loss 0.007, Val loss 7.517\n",
            "Ep 29 (Step 000260): Train loss 0.005, Val loss 7.536\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 30 (Step 000265): Train loss 0.003, Val loss 7.507\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 31 (Step 000270): Train loss 0.003, Val loss 7.442\n",
            "Ep 31 (Step 000275): Train loss 0.006, Val loss 7.479\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 32 (Step 000280): Train loss 0.003, Val loss 7.565\n",
            "Ep 32 (Step 000285): Train loss 0.003, Val loss 7.595\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 33 (Step 000290): Train loss 0.002, Val loss 7.580\n",
            "Ep 33 (Step 000295): Train loss 0.002, Val loss 7.564\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 34 (Step 000300): Train loss 0.002, Val loss 7.552\n",
            "Ep 34 (Step 000305): Train loss 0.002, Val loss 7.551\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 35 (Step 000310): Train loss 0.002, Val loss 7.565\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 36 (Step 000315): Train loss 0.002, Val loss 7.584\n",
            "Ep 36 (Step 000320): Train loss 0.002, Val loss 7.591\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 37 (Step 000325): Train loss 0.004, Val loss 7.603\n",
            "Ep 37 (Step 000330): Train loss 0.002, Val loss 7.664\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 38 (Step 000335): Train loss 0.003, Val loss 7.638\n",
            "Ep 38 (Step 000340): Train loss 0.003, Val loss 7.710\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 39 (Step 000345): Train loss 0.002, Val loss 7.644\n",
            "Ep 39 (Step 000350): Train loss 0.002, Val loss 7.640\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 40 (Step 000355): Train loss 0.002, Val loss 7.616\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 41 (Step 000360): Train loss 0.005, Val loss 7.639\n",
            "Ep 41 (Step 000365): Train loss 0.009, Val loss 7.589\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 42 (Step 000370): Train loss 0.003, Val loss 7.605\n",
            "Ep 42 (Step 000375): Train loss 0.007, Val loss 7.631\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 43 (Step 000380): Train loss 0.008, Val loss 7.603\n",
            "Ep 43 (Step 000385): Train loss 0.015, Val loss 7.630\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 44 (Step 000390): Train loss 0.011, Val loss 7.566\n",
            "Ep 44 (Step 000395): Train loss 0.004, Val loss 7.606\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 45 (Step 000400): Train loss 0.014, Val loss 7.712\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 46 (Step 000405): Train loss 0.016, Val loss 7.669\n",
            "Ep 46 (Step 000410): Train loss 0.018, Val loss 7.582\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 47 (Step 000415): Train loss 0.018, Val loss 7.580\n",
            "Ep 47 (Step 000420): Train loss 0.024, Val loss 7.594\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 48 (Step 000425): Train loss 0.015, Val loss 7.706\n",
            "Ep 48 (Step 000430): Train loss 0.017, Val loss 7.669\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 49 (Step 000435): Train loss 0.013, Val loss 7.548\n",
            "Ep 49 (Step 000440): Train loss 0.009, Val loss 7.598\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Ep 50 (Step 000445): Train loss 0.009, Val loss 7.554\n",
            "Every effort moves you?\"  \"Yes--quite insensible to the irony. She wanted him vindicated--and by me!\"  He laughed again, and threw back his head to look up at the sketch of the donkey. \"There were days when I\n",
            "Training completed in 1.32 minutes.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 500x300 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "!pip install tiktoken\n",
        "import importlib\n",
        "import tiktoken\n",
        "print('tiktoken version : ',importlib.metadata.version('tiktoken'))\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "print(\"Device name:\", torch.cuda.get_device_name(0))\n",
        "print(\"Device:\", device)\n",
        "\n",
        "with open('/content/drive/MyDrive/Large Language Models/data/verdict.txt','r',encoding='utf-8') as f:\n",
        "  text_data=f.read()\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input_ids=[]\n",
        "    self.target_ids=[]\n",
        "\n",
        "    token_ids=tokenizer.encode(txt,allowed_special={'<|endoftext|>'})\n",
        "\n",
        "    for i in range(0,len(token_ids)-max_length,stride):\n",
        "      input_chunk=token_ids[i:i+max_length]\n",
        "      target_chunk=token_ids[i+1:i+max_length+1]\n",
        "      self.input_ids.append(torch.tensor(input_chunk))\n",
        "      self.target_ids.append(torch.tensor(target_chunk))\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "  def __getitem__(self,idx):\n",
        "    return self.input_ids[idx],self.target_ids[idx]\n",
        "\n",
        "\n",
        "def create_dataloader_v1(txt,batch_size=4,max_length=256,stride=128,shuffle=True,drop_last=True,num_workers=0):\n",
        "  tokenizer=tiktoken.get_encoding('gpt2')\n",
        "  dataset=GPTDatasetV1(txt,tokenizer,max_length,stride)\n",
        "\n",
        "  dataloader=DataLoader(\n",
        "      dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader\n",
        "GPT_CONDIG_124M={\n",
        "    'vocab_size':50257,\n",
        "    'context_length':256,\n",
        "    'emb_dim':768,\n",
        "    'n_heads':12,\n",
        "    'n_layers':12,\n",
        "    'drop_rate':0.1,\n",
        "    'qkv_bias':False\n",
        "}\n",
        "train_ratio=0.90\n",
        "split_idx=int(train_ratio*len(text_data))\n",
        "train_data=text_data[:split_idx]\n",
        "val_data=text_data[split_idx:]\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader=create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONDIG_124M['context_length'],\n",
        "    stride=GPT_CONDIG_124M['context_length'],\n",
        "    shuffle=True,\n",
        "    drop_last=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader=create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONDIG_124M['context_length'],\n",
        "    stride=GPT_CONDIG_124M['context_length'],\n",
        "    shuffle=False,\n",
        "    drop_last=False,\n",
        "    num_workers=0\n",
        ")\n",
        "print(\"Train loader:\")\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(\"\\nValidation loader:\")\n",
        "for x, y in val_loader:\n",
        "    print(x.shape, y.shape)\n",
        "\n",
        "print(len(train_loader))\n",
        "print(len(val_loader))\n",
        "\n",
        "train_tokens = 0\n",
        "for input_batch, target_batch in train_loader:\n",
        "    train_tokens += input_batch.numel()\n",
        "\n",
        "val_tokens = 0\n",
        "for input_batch, target_batch in val_loader:\n",
        "    val_tokens += input_batch.numel()\n",
        "\n",
        "print(\"Training tokens:\", train_tokens)\n",
        "print(\"Validation tokens:\", val_tokens)\n",
        "print(\"All tokens:\", train_tokens + val_tokens)\n",
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,context_length,dropout,num_heads,qkv_bias=False):\n",
        "    super().__init__()\n",
        "    assert (d_out % num_heads==0),\\\n",
        "      'd_out must be divisible by num_heads'\n",
        "\n",
        "    self.d_out=d_out\n",
        "    self.num_heads=num_heads\n",
        "    self.head_dim=d_out//num_heads\n",
        "\n",
        "    self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.out_proj=nn.Linear(d_out,d_out)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length),diagonal=1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_tokens,d_in=x.shape\n",
        "\n",
        "    keys=self.W_key(x)  #shape:(b,num_tokens,d_out)(2,4,6 (num_heads==2))\n",
        "    queries=self.W_query(x)\n",
        "    values=self.W_value(x)\n",
        "\n",
        "    #(b,num_tokens,d_out)>(b,num_tokens,num_heads,head_dim)(2,4,6 > 2,4,2,3)\n",
        "    keys=keys.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    queries=queries.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "    values=values.view(b,num_tokens,self.num_heads,self.head_dim)\n",
        "\n",
        "    keys=keys.transpose(1,2)   #(b,num_tokens,num_heads,head_dim)>(b,num_heads,num_tokens,head_dim)\n",
        "    values=values.transpose(1,2)\n",
        "    queries=queries.transpose(1,2)\n",
        "\n",
        "    attn_scores=queries @ keys.transpose(2,3) #(b,num_heads,num_tokens,head_dim) @ ((b,num_heads,head_dim,num_tokens))\n",
        "    #attn_score.shape=(b,num_heads,num_tokens,num_tokens)\n",
        "\n",
        "    mask_bool=self.mask.bool()[:num_tokens,:num_tokens] #Applying masking\n",
        "    attn_scores = attn_scores.masked_fill(mask_bool, -torch.inf)\n",
        "\n",
        "\n",
        "\n",
        "    attn_weights=torch.softmax(attn_scores/keys.shape[-1]**0.5,dim=-1) #applying scale and softmax\n",
        "    attn_weights=self.dropout(attn_weights)\n",
        "\n",
        "    context_vec=(attn_weights@values).transpose(1,2) #-->(b,num_tokens,num_heads,head_dim)\n",
        "    context_vec=context_vec.contiguous().view(b,num_tokens,self.d_out) #(b,num_tokens,self.d_out)\n",
        "    context_vec=self.out_proj(context_vec)\n",
        "\n",
        "    return context_vec\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "  def __init__(self,emb_dim):\n",
        "    super().__init__()\n",
        "    self.eps=1e-5\n",
        "    self.scale=nn.Parameter(torch.ones(emb_dim))\n",
        "    self.shift=nn.Parameter(torch.zeros(emb_dim))\n",
        "  def forward(self,x):\n",
        "    mean=x.mean(dim=-1,keepdim=True) #dim=-1 for columns keepdim = keep dimensions\n",
        "    var=x.var(dim=-1,keepdim=True,unbiased=False) #unbiased=False = // n    not n-1 for var\n",
        "    norm_x=(x-mean)/torch.sqrt(var+self.eps)\n",
        "    return self.scale*norm_x+self.shift\n",
        "class Gelu(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return 0.5*x*(1+torch.tanh(torch.sqrt(torch.tensor(2.0/torch.pi))*(x+0.044715*torch.pow(x,3))))\n",
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.layers=nn.Sequential(\n",
        "        nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
        "        Gelu(),\n",
        "        nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.layers(x)\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.att=MultiHeadAttention(\n",
        "        d_in=cfg['emb_dim'],\n",
        "        d_out=cfg['emb_dim'],\n",
        "        context_length=cfg['context_length'],\n",
        "        num_heads=cfg['n_heads'],\n",
        "        dropout=cfg['drop_rate'],\n",
        "        qkv_bias=cfg['qkv_bias'])\n",
        "    self.ff=FeedForward(cfg)\n",
        "    self.norm1=LayerNorm(cfg['emb_dim'])\n",
        "    self.norm2=LayerNorm(cfg['emb_dim'])\n",
        "    self.drop_shotcut=nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "  def forward(self,x):\n",
        "    shortcut=x\n",
        "    x=self.norm1(x)\n",
        "    x=self.att(x)\n",
        "    x=self.drop_shotcut(x)\n",
        "    x=x+shortcut\n",
        "\n",
        "    shotcut=x\n",
        "    x=self.norm2(x)\n",
        "    x=self.ff(x)\n",
        "    x=self.drop_shotcut(x)\n",
        "    x=x+shortcut\n",
        "\n",
        "    return x\n",
        "class GPTModel(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.tok_emb=nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
        "    self.pos_emb=nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
        "    self.drop_emb=nn.Dropout(cfg['drop_rate'])\n",
        "    self.trf_blocks=nn.Sequential(\n",
        "        *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "    self.final_norm=LayerNorm(cfg['emb_dim'])\n",
        "    self.out_head=nn.Linear(cfg['emb_dim'],cfg['vocab_size'],bias=False)\n",
        "  def forward(self,in_idx):\n",
        "    batch_size,seq_length=in_idx.shape\n",
        "    tok_embeds=self.tok_emb(in_idx)\n",
        "    pos_embeds=self.pos_emb(torch.arange(seq_length,device=in_idx.device))\n",
        "    x=tok_embeds+pos_embeds\n",
        "    x=self.drop_emb(x)\n",
        "    x=self.trf_blocks(x)\n",
        "    x=self.final_norm(x)\n",
        "    logits=self.out_head(x)\n",
        "    return logits\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model=GPTModel(GPT_CONDIG_124M)\n",
        "model.eval()\n",
        "import torch\n",
        "import tiktoken\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "tokenizer = tiktoken.get_encoding('gpt2')\n",
        "\n",
        "# --- Text <-> Token Fonksiyonları ---\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0)  # (batch_size, seq_len)\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0)  # (seq_len,)\n",
        "    return tokenizer.decode(flat.tolist())\n",
        "\n",
        "# --- Gelişmiş Generate Fonksiyonu ---\n",
        "def generate(model, idx, max_new_tokens, context_size, temperature=0.0, top_k=None, eos_id=None):\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :]  # Son token prediction\n",
        "\n",
        "        if top_k is not None:\n",
        "            top_logits, _ = torch.topk(logits, top_k)\n",
        "            min_val = top_logits[:, -1].unsqueeze(-1)  # dikkat! batch boyutuna göre\n",
        "            logits = torch.where(logits < min_val, torch.full_like(logits, float('-inf')), logits)\n",
        "\n",
        "        if temperature > 0.0:\n",
        "            logits = logits / temperature\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # sampling\n",
        "        else:\n",
        "            idx_next = torch.argmax(logits, dim=-1, keepdim=True)  # greedy\n",
        "\n",
        "        if eos_id is not None:\n",
        "            if (idx_next == eos_id).all():\n",
        "                break\n",
        "\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "\n",
        "\n",
        "def calc_loss_batch(input_batch,target_batch,model,device):\n",
        "  input_batch,target_batch=input_batch.to(device),target_batch.to(device)\n",
        "  logits=model(input_batch)\n",
        "  logits=logits.flatten(0,1)\n",
        "  targets=target_batch.flatten()\n",
        "  loss=torch.nn.functional.cross_entropy(logits,targets)\n",
        "  return loss\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "\n",
        "    if len(data_loader) == 0:\n",
        "        return float('nan')\n",
        "\n",
        "    if num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "\n",
        "    with torch.no_grad():  # Validation sırasında gradient hesabı yapmaya gerek yok\n",
        "        for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "            if i < num_batches:\n",
        "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                total_loss += loss.item()\n",
        "            else:\n",
        "                break\n",
        "\n",
        "    model.train()  # Tekrar train moduna alıyoruz\n",
        "    return total_loss / num_batches\n",
        "def train_model_simple(model,train_loader,val_loader,optimizer,device,num_epochs,\n",
        "                        eval_freq,eval_iter,start_context,tokenizer):\n",
        "  train_losses,val_losses,track_tokens_seen=[],[],[]\n",
        "  tokens_seen,global_step=0,-1\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for input_batch,target_batch in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      loss=calc_loss_batch(input_batch,target_batch,model,device)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      tokens_seen+=input_batch.numel()\n",
        "      global_step +=1\n",
        "\n",
        "      if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "    generate_and_print_sample(model,tokenizer,device,start_context)\n",
        "  return train_losses, val_losses, track_tokens_seen\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "    decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "    print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()\n",
        "# Note:\n",
        "# Uncomment the following code to calculate the execution time\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONDIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 50\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"Every effort moves you\", tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Note:\n",
        "# Uncomment the following code to show the execution time\n",
        "end_time = time.time()\n",
        "execution_time_minutes = (end_time - start_time) / 60\n",
        "print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "\n",
        "def plot_losses(epochs_seen, tokens_seen, train_losses, val_losses):\n",
        "    fig, ax1 = plt.subplots(figsize=(5, 3))\n",
        "\n",
        "    # Plot training and validation loss against epochs\n",
        "    ax1.plot(epochs_seen, train_losses, label=\"Training loss\")\n",
        "    ax1.plot(epochs_seen, val_losses, linestyle=\"-.\", label=\"Validation loss\")\n",
        "    ax1.set_xlabel(\"Epochs\")\n",
        "    ax1.set_ylabel(\"Loss\")\n",
        "    ax1.legend(loc=\"upper right\")\n",
        "    ax1.xaxis.set_major_locator(MaxNLocator(integer=True))  # only show integer labels on x-axis\n",
        "\n",
        "    # Create a second x-axis for tokens seen\n",
        "    ax2 = ax1.twiny()  # Create a second x-axis that shares the same y-axis\n",
        "    ax2.plot(tokens_seen, train_losses, alpha=0)  # Invisible plot for aligning ticks\n",
        "    ax2.set_xlabel(\"Tokens seen\")\n",
        "\n",
        "    fig.tight_layout()  # Adjust layout to make room\n",
        "    plt.savefig(\"loss-plot.pdf\")\n",
        "    plt.show()\n",
        "\n",
        "epochs_tensor = torch.linspace(0, num_epochs, len(train_losses))\n",
        "plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqRSU44Szrd1",
        "outputId": "c3414ab7-05de-4459-fd9e-78454a4f8ea5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Save checkpoint (Google Drive'a) ---\n",
        "checkpoint = {\n",
        "    'epoch': num_epochs,  # İstersen son epoch'u da kaydedebilirsin\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, '/content/drive/MyDrive/gptmodel_checkpoint.pth')\n",
        "print(\"Checkpoint saved to Drive!\")\n",
        "\n",
        "# --- Save full model (Google Drive'a) ---\n",
        "torch.save(model, '/content/drive/MyDrive/gptmodel_full.pth')\n",
        "print(\"Full model saved to Drive!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaCO4HMlzsIA",
        "outputId": "1fe292a6-3e8c-4c00-bf8a-d5a20225198b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint saved to Drive!\n",
            "Full model saved to Drive!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # 1. Model mimarisini tekrar oluştur\n",
        "# model = GPTModel(GPT_CONDIG_124M)  # dikkat: config doğru olmalı\n",
        "# model.to(device)\n",
        "\n",
        "# # 2. Checkpoint'i yükle\n",
        "# checkpoint = torch.load('/content/drive/MyDrive/gptmodel_checkpoint.pth', map_location=device)\n",
        "\n",
        "# # 3. Model ağırlıklarını yükle\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# # 4. Optimizer'ı tekrar oluştur\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "# # 5. Optimizer ağırlıklarını yükle\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "# # 6. (Opsiyonel) Epoch bilgisini oku\n",
        "# start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "# print(\"Model ve optimizer başarıyla yüklendi! Eğitim {}. epoch'tan devam edebilir.\".format(start_epoch))\n",
        "\n",
        "\n",
        "# # 1. Tüm modeli yükle\n",
        "# model = torch.load('/content/drive/MyDrive/gptmodel_full.pth', map_location=device)\n",
        "\n",
        "# # 2. Modeli device'a taşı\n",
        "# model.to(device)\n",
        "\n",
        "# # 3. Optimizer'ı tekrar oluştur (çünkü optimizer kaydedilmemiştir full modelde)\n",
        "# optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "# print(\"Full model başarıyla yüklendi!\")\n"
      ],
      "metadata": {
        "id": "i4VVoADM3Fqt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}